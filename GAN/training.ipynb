{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from model import Discriminator, Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "lr = 3e-4\n",
    "z_dim = 64\n",
    "image_dim = 28 * 28 * 1\n",
    "batch_size = 32\n",
    "num_epochs = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc = Discriminator(image_dim).to(device)\n",
    "gen = Generator(z_dim, image_dim).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixed noise for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_noise = torch.rand((batch_size, z_dim)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.MNIST(root=\"dataset/\", transform=transforms, download=True)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer, Loss and Metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_disc = optim.Adam(disc.parameters(), lr=lr)\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=lr)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorboard stuff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_fake = SummaryWriter(f\"runs/GAN_MNIST/fake\")\n",
    "writer_real = SummaryWriter(f\"runs/GAN_MNIST/real\")\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/200] Batch 0/1875                   Loss D: 0.6808, loss G: 0.6816\n",
      "Epoch [1/200] Batch 0/1875                   Loss D: 0.0116, loss G: 4.7049\n",
      "Epoch [2/200] Batch 0/1875                   Loss D: 0.0005, loss G: 7.4484\n",
      "Epoch [3/200] Batch 0/1875                   Loss D: 0.0003, loss G: 8.2157\n",
      "Epoch [4/200] Batch 0/1875                   Loss D: 0.0000, loss G: 11.0840\n",
      "Epoch [5/200] Batch 0/1875                   Loss D: 0.0000, loss G: 12.8336\n",
      "Epoch [6/200] Batch 0/1875                   Loss D: 0.0000, loss G: 13.6879\n",
      "Epoch [7/200] Batch 0/1875                   Loss D: 0.0000, loss G: 15.0445\n",
      "Epoch [8/200] Batch 0/1875                   Loss D: 0.0000, loss G: 17.0174\n",
      "Epoch [9/200] Batch 0/1875                   Loss D: 0.0000, loss G: 17.8752\n",
      "Epoch [10/200] Batch 0/1875                   Loss D: 0.0000, loss G: 16.8189\n",
      "Epoch [11/200] Batch 0/1875                   Loss D: 0.0000, loss G: 18.2589\n",
      "Epoch [12/200] Batch 0/1875                   Loss D: 0.0000, loss G: 19.4139\n",
      "Epoch [13/200] Batch 0/1875                   Loss D: 0.0000, loss G: 25.1858\n",
      "Epoch [14/200] Batch 0/1875                   Loss D: 0.0000, loss G: 19.0982\n",
      "Epoch [15/200] Batch 0/1875                   Loss D: 0.0000, loss G: 17.4999\n",
      "Epoch [16/200] Batch 0/1875                   Loss D: 0.0000, loss G: 11.6112\n",
      "Epoch [17/200] Batch 0/1875                   Loss D: 0.0000, loss G: 13.3796\n",
      "Epoch [18/200] Batch 0/1875                   Loss D: 0.0000, loss G: 12.0144\n",
      "Epoch [19/200] Batch 0/1875                   Loss D: 0.0000, loss G: 14.8447\n",
      "Epoch [20/200] Batch 0/1875                   Loss D: 0.0000, loss G: 16.5434\n",
      "Epoch [21/200] Batch 0/1875                   Loss D: 0.0000, loss G: 16.9800\n",
      "Epoch [22/200] Batch 0/1875                   Loss D: 0.0000, loss G: 18.5299\n",
      "Epoch [23/200] Batch 0/1875                   Loss D: 0.0000, loss G: 18.8952\n",
      "Epoch [24/200] Batch 0/1875                   Loss D: 0.0000, loss G: 17.0982\n",
      "Epoch [25/200] Batch 0/1875                   Loss D: 0.0000, loss G: 19.2396\n",
      "Epoch [26/200] Batch 0/1875                   Loss D: 0.0000, loss G: 15.5073\n",
      "Epoch [27/200] Batch 0/1875                   Loss D: 0.0000, loss G: 18.5077\n",
      "Epoch [28/200] Batch 0/1875                   Loss D: 0.0000, loss G: 19.6912\n",
      "Epoch [29/200] Batch 0/1875                   Loss D: 0.0000, loss G: 20.0382\n",
      "Epoch [30/200] Batch 0/1875                   Loss D: 0.0000, loss G: 19.0982\n",
      "Epoch [31/200] Batch 0/1875                   Loss D: 0.0000, loss G: 21.5036\n",
      "Epoch [32/200] Batch 0/1875                   Loss D: 0.0000, loss G: 22.2457\n",
      "Epoch [33/200] Batch 0/1875                   Loss D: 0.0000, loss G: 23.0630\n",
      "Epoch [34/200] Batch 0/1875                   Loss D: 0.0000, loss G: 22.9148\n",
      "Epoch [35/200] Batch 0/1875                   Loss D: 0.0000, loss G: 17.4592\n",
      "Epoch [36/200] Batch 0/1875                   Loss D: 0.0000, loss G: 18.6792\n",
      "Epoch [37/200] Batch 0/1875                   Loss D: 0.0000, loss G: 20.2850\n",
      "Epoch [38/200] Batch 0/1875                   Loss D: 0.0000, loss G: 20.2177\n",
      "Epoch [39/200] Batch 0/1875                   Loss D: 0.0000, loss G: 13.3456\n",
      "Epoch [40/200] Batch 0/1875                   Loss D: 0.0000, loss G: 15.5752\n",
      "Epoch [41/200] Batch 0/1875                   Loss D: 0.0000, loss G: 16.4298\n",
      "Epoch [42/200] Batch 0/1875                   Loss D: 0.0000, loss G: 17.4672\n",
      "Epoch [43/200] Batch 0/1875                   Loss D: 0.0000, loss G: 18.2114\n",
      "Epoch [44/200] Batch 0/1875                   Loss D: 0.0000, loss G: 23.6503\n",
      "Epoch [45/200] Batch 0/1875                   Loss D: 0.0000, loss G: 21.0734\n",
      "Epoch [46/200] Batch 0/1875                   Loss D: 0.0000, loss G: 19.7467\n",
      "Epoch [47/200] Batch 0/1875                   Loss D: 0.0000, loss G: 18.8271\n",
      "Epoch [48/200] Batch 0/1875                   Loss D: 0.0000, loss G: 19.2692\n",
      "Epoch [49/200] Batch 0/1875                   Loss D: 0.0000, loss G: 12.6838\n",
      "Epoch [50/200] Batch 0/1875                   Loss D: 0.0000, loss G: 15.2745\n",
      "Epoch [51/200] Batch 0/1875                   Loss D: 0.0000, loss G: 15.8670\n",
      "Epoch [52/200] Batch 0/1875                   Loss D: 0.0000, loss G: 17.1342\n",
      "Epoch [53/200] Batch 0/1875                   Loss D: 0.0000, loss G: 18.0237\n",
      "Epoch [54/200] Batch 0/1875                   Loss D: 0.0000, loss G: 18.8829\n",
      "Epoch [55/200] Batch 0/1875                   Loss D: 0.0000, loss G: 20.1661\n",
      "Epoch [56/200] Batch 0/1875                   Loss D: 0.0000, loss G: 21.0011\n",
      "Epoch [57/200] Batch 0/1875                   Loss D: 0.0000, loss G: 21.9180\n",
      "Epoch [58/200] Batch 0/1875                   Loss D: 0.0000, loss G: 20.0788\n",
      "Epoch [59/200] Batch 0/1875                   Loss D: 0.0000, loss G: 18.7078\n",
      "Epoch [60/200] Batch 0/1875                   Loss D: 0.0000, loss G: 20.4148\n",
      "Epoch [61/200] Batch 0/1875                   Loss D: 0.0000, loss G: 22.0905\n",
      "Epoch [62/200] Batch 0/1875                   Loss D: 0.0000, loss G: 22.7648\n",
      "Epoch [63/200] Batch 0/1875                   Loss D: 0.0000, loss G: 11.8681\n",
      "Epoch [64/200] Batch 0/1875                   Loss D: 0.0012, loss G: 6.5390\n",
      "Epoch [65/200] Batch 0/1875                   Loss D: 0.0000, loss G: 12.6147\n",
      "Epoch [66/200] Batch 0/1875                   Loss D: 0.0000, loss G: 14.8807\n",
      "Epoch [67/200] Batch 0/1875                   Loss D: 0.0000, loss G: 15.1812\n",
      "Epoch [68/200] Batch 0/1875                   Loss D: 0.0000, loss G: 17.6800\n",
      "Epoch [69/200] Batch 0/1875                   Loss D: 0.0000, loss G: 17.5458\n",
      "Epoch [70/200] Batch 0/1875                   Loss D: 0.0000, loss G: 16.1678\n",
      "Epoch [71/200] Batch 0/1875                   Loss D: 0.0000, loss G: 16.8292\n",
      "Epoch [72/200] Batch 0/1875                   Loss D: 0.0000, loss G: 16.9270\n",
      "Epoch [73/200] Batch 0/1875                   Loss D: 0.0000, loss G: 18.1436\n",
      "Epoch [74/200] Batch 0/1875                   Loss D: 0.0000, loss G: 11.4255\n",
      "Epoch [75/200] Batch 0/1875                   Loss D: 0.0000, loss G: 13.7977\n",
      "Epoch [76/200] Batch 0/1875                   Loss D: 0.0000, loss G: 14.7830\n",
      "Epoch [77/200] Batch 0/1875                   Loss D: 0.0000, loss G: 15.7409\n",
      "Epoch [78/200] Batch 0/1875                   Loss D: 0.0000, loss G: 16.2531\n",
      "Epoch [79/200] Batch 0/1875                   Loss D: 0.0000, loss G: 18.4437\n",
      "Epoch [80/200] Batch 0/1875                   Loss D: 0.0000, loss G: 19.2761\n",
      "Epoch [81/200] Batch 0/1875                   Loss D: 0.0000, loss G: 20.0108\n",
      "Epoch [82/200] Batch 0/1875                   Loss D: 0.0000, loss G: 21.0603\n",
      "Epoch [83/200] Batch 0/1875                   Loss D: 0.0000, loss G: 21.2320\n",
      "Epoch [84/200] Batch 0/1875                   Loss D: 0.0000, loss G: 22.0716\n",
      "Epoch [85/200] Batch 0/1875                   Loss D: 0.0000, loss G: 22.9947\n",
      "Epoch [86/200] Batch 0/1875                   Loss D: 0.0000, loss G: 22.7025\n",
      "Epoch [87/200] Batch 0/1875                   Loss D: 0.0000, loss G: 23.6412\n",
      "Epoch [88/200] Batch 0/1875                   Loss D: 0.0000, loss G: 24.7815\n",
      "Epoch [89/200] Batch 0/1875                   Loss D: 0.0000, loss G: 23.5052\n",
      "Epoch [90/200] Batch 0/1875                   Loss D: 0.0000, loss G: 16.1561\n",
      "Epoch [91/200] Batch 0/1875                   Loss D: 0.0000, loss G: 18.9247\n",
      "Epoch [92/200] Batch 0/1875                   Loss D: 0.0000, loss G: 20.4546\n",
      "Epoch [93/200] Batch 0/1875                   Loss D: 0.0000, loss G: 21.4160\n",
      "Epoch [94/200] Batch 0/1875                   Loss D: 0.0000, loss G: 21.4240\n",
      "Epoch [95/200] Batch 0/1875                   Loss D: 0.0000, loss G: 21.3167\n",
      "Epoch [96/200] Batch 0/1875                   Loss D: 0.0000, loss G: 22.7378\n",
      "Epoch [97/200] Batch 0/1875                   Loss D: 0.0000, loss G: 23.8810\n",
      "Epoch [98/200] Batch 0/1875                   Loss D: 0.0000, loss G: 24.2857\n",
      "Epoch [99/200] Batch 0/1875                   Loss D: 0.0000, loss G: 13.7561\n",
      "Epoch [100/200] Batch 0/1875                   Loss D: 0.0000, loss G: 14.6594\n",
      "Epoch [101/200] Batch 0/1875                   Loss D: 0.0000, loss G: 17.2960\n",
      "Epoch [102/200] Batch 0/1875                   Loss D: 0.0000, loss G: 13.4410\n",
      "Epoch [103/200] Batch 0/1875                   Loss D: 0.0000, loss G: 20.3944\n",
      "Epoch [104/200] Batch 0/1875                   Loss D: 0.0000, loss G: 20.9073\n",
      "Epoch [105/200] Batch 0/1875                   Loss D: 0.0000, loss G: 21.4073\n",
      "Epoch [106/200] Batch 0/1875                   Loss D: 0.0000, loss G: 22.1121\n",
      "Epoch [107/200] Batch 0/1875                   Loss D: 0.0000, loss G: 22.8108\n",
      "Epoch [108/200] Batch 0/1875                   Loss D: 0.0000, loss G: 23.5664\n",
      "Epoch [109/200] Batch 0/1875                   Loss D: 0.0000, loss G: 23.8778\n",
      "Epoch [110/200] Batch 0/1875                   Loss D: 0.0000, loss G: 24.7048\n",
      "Epoch [111/200] Batch 0/1875                   Loss D: 0.0000, loss G: 24.9358\n",
      "Epoch [112/200] Batch 0/1875                   Loss D: 0.0000, loss G: 25.4505\n",
      "Epoch [113/200] Batch 0/1875                   Loss D: 0.0000, loss G: 25.8175\n",
      "Epoch [114/200] Batch 0/1875                   Loss D: 0.0000, loss G: 26.0849\n",
      "Epoch [115/200] Batch 0/1875                   Loss D: 0.0000, loss G: 26.0956\n",
      "Epoch [116/200] Batch 0/1875                   Loss D: 0.0000, loss G: 26.4828\n",
      "Epoch [117/200] Batch 0/1875                   Loss D: 0.0000, loss G: 26.7110\n",
      "Epoch [118/200] Batch 0/1875                   Loss D: 0.0000, loss G: 27.0347\n",
      "Epoch [119/200] Batch 0/1875                   Loss D: 0.0000, loss G: 26.9159\n",
      "Epoch [120/200] Batch 0/1875                   Loss D: 0.0000, loss G: 27.0505\n",
      "Epoch [121/200] Batch 0/1875                   Loss D: 0.0000, loss G: 27.1043\n",
      "Epoch [122/200] Batch 0/1875                   Loss D: 0.0000, loss G: 27.0368\n",
      "Epoch [123/200] Batch 0/1875                   Loss D: 0.0000, loss G: 27.0272\n",
      "Epoch [124/200] Batch 0/1875                   Loss D: 0.0000, loss G: 27.0709\n",
      "Epoch [125/200] Batch 0/1875                   Loss D: 0.0000, loss G: 26.9832\n",
      "Epoch [126/200] Batch 0/1875                   Loss D: 0.0000, loss G: 27.0466\n",
      "Epoch [127/200] Batch 0/1875                   Loss D: 0.0000, loss G: 27.0303\n",
      "Epoch [128/200] Batch 0/1875                   Loss D: 0.0000, loss G: 27.5652\n",
      "Epoch [129/200] Batch 0/1875                   Loss D: 0.0000, loss G: 27.3803\n",
      "Epoch [130/200] Batch 0/1875                   Loss D: 0.0000, loss G: 27.7752\n",
      "Epoch [131/200] Batch 0/1875                   Loss D: 0.0000, loss G: 27.4901\n",
      "Epoch [132/200] Batch 0/1875                   Loss D: 0.0000, loss G: 27.8378\n",
      "Epoch [133/200] Batch 0/1875                   Loss D: 0.0000, loss G: 27.6001\n",
      "Epoch [134/200] Batch 0/1875                   Loss D: 0.0000, loss G: 27.9003\n",
      "Epoch [135/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.0853\n",
      "Epoch [136/200] Batch 0/1875                   Loss D: 0.0000, loss G: 27.6918\n",
      "Epoch [137/200] Batch 0/1875                   Loss D: 0.0000, loss G: 27.9500\n",
      "Epoch [138/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.1193\n",
      "Epoch [139/200] Batch 0/1875                   Loss D: 0.0000, loss G: 27.6762\n",
      "Epoch [140/200] Batch 0/1875                   Loss D: 0.0000, loss G: 27.9400\n",
      "Epoch [141/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.1120\n",
      "Epoch [142/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.2366\n",
      "Epoch [143/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.3356\n",
      "Epoch [144/200] Batch 0/1875                   Loss D: 0.0000, loss G: 27.9250\n",
      "Epoch [145/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.1003\n",
      "Epoch [146/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.2276\n",
      "Epoch [147/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.3282\n",
      "Epoch [148/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.4118\n",
      "Epoch [149/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.4838\n",
      "Epoch [150/200] Batch 0/1875                   Loss D: 0.0000, loss G: 27.9668\n",
      "Epoch [151/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.1279\n",
      "Epoch [152/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.2477\n",
      "Epoch [153/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.3446\n",
      "Epoch [154/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.4255\n",
      "Epoch [155/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.4958\n",
      "Epoch [156/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.5554\n",
      "Epoch [157/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.6062\n",
      "Epoch [158/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.0582\n",
      "Epoch [159/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.1931\n",
      "Epoch [160/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.2986\n",
      "Epoch [161/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.3866\n",
      "Epoch [162/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.4618\n",
      "Epoch [163/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.5269\n",
      "Epoch [164/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.5816\n",
      "Epoch [165/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.6293\n",
      "Epoch [166/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.6712\n",
      "Epoch [167/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.1833\n",
      "Epoch [168/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.2893\n",
      "Epoch [169/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.3784\n",
      "Epoch [170/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.4545\n",
      "Epoch [171/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.5206\n",
      "Epoch [172/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.5763\n",
      "Epoch [173/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.6247\n",
      "Epoch [174/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.6671\n",
      "Epoch [175/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.7059\n",
      "Epoch [176/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.7411\n",
      "Epoch [177/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.7739\n",
      "Epoch [178/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.8046\n",
      "Epoch [179/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.2459\n",
      "Epoch [180/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.3406\n",
      "Epoch [181/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.4213\n",
      "Epoch [182/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.4919\n",
      "Epoch [183/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.5521\n",
      "Epoch [184/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.6032\n",
      "Epoch [185/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.6482\n",
      "Epoch [186/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.6885\n",
      "Epoch [187/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.7253\n",
      "Epoch [188/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.7592\n",
      "Epoch [189/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.7908\n",
      "Epoch [190/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.8207\n",
      "Epoch [191/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.8487\n",
      "Epoch [192/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.8751\n",
      "Epoch [193/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.3445\n",
      "Epoch [194/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.4228\n",
      "Epoch [195/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.4924\n",
      "Epoch [196/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.5523\n",
      "Epoch [197/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.6034\n",
      "Epoch [198/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.6483\n",
      "Epoch [199/200] Batch 0/1875                   Loss D: 0.0000, loss G: 28.6885\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (real, _) in enumerate(loader):\n",
    "        # keep the batch size the same, but flatten the images\n",
    "        real = real.view(-1, 784).to(device)\n",
    "        batch_size = real.shape[0]\n",
    "\n",
    "        # Train Discriminator: max log(D(real)) + log(1 - D(G(z)))\n",
    "        noise = torch.rand(batch_size, z_dim).to(device)\n",
    "        fake = gen(noise)\n",
    "        disc_real = disc(real).view(-1)\n",
    "        # BCE: -wn * (y * log(x) + (1 - y) * log(1 - x))\n",
    "        # if we set y to 1, we get -wn * log(x)\n",
    "        # and maximizing log(x) is the same as minimizing -log(x)\n",
    "        # and we normally want to minimize the loss function\n",
    "        lossD_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "        disc_fake = disc(fake).view(-1)\n",
    "        # BCE: -wn * (y * log(x) + (1 - y) * log(1 - x))\n",
    "        # if we set y to 0, we get -wn * log(1 - x)\n",
    "        # and maximizing log(1 - x) is the same as minimizing -log(1 - x)\n",
    "        lossD_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "        lossD = (lossD_real + lossD_fake) / 2\n",
    "        disc.zero_grad()\n",
    "        # retain_graph=True because we want to reuse the fake variable again which is part of the computational graph\n",
    "        lossD.backward(retain_graph=True)\n",
    "        opt_disc.step()\n",
    "\n",
    "        # Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
    "        output = disc(fake).view(-1)\n",
    "        lossG = criterion(output, torch.ones_like(output))\n",
    "        gen.zero_grad()\n",
    "        lossG.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        if batch_idx == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(loader)} \\\n",
    "                  Loss D: {lossD:.4f}, loss G: {lossG:.4f}\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = gen(fixed_noise).reshape(-1, 1, 28, 28)\n",
    "                data = real.reshape(-1, 1, 28, 28)\n",
    "                img_grid_fake = torchvision.utils.make_grid(fake, normalize=True)\n",
    "                img_grid_real = torchvision.utils.make_grid(data, normalize=True)\n",
    "\n",
    "                writer_fake.add_image(\"Mnist Fake Images\", img_grid_fake, global_step=step)\n",
    "                writer_real.add_image(\"Mnist Real Images\", img_grid_real, global_step=step)\n",
    "                step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
